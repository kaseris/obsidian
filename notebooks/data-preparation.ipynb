{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1adfb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f04720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_FASHION_DIR = osp.join(osp.expanduser('~'), 'Documents', 'dev', 'DeepFashion')\n",
    "DEEP_FASHION_CLOTHING_ANNOS_DIR = osp.join(DEEP_FASHION_DIR, 'Anno_coarse')\n",
    "DEEP_FASHION_CLOTHING_IMAGES_DIR = osp.join(DEEP_FASHION_DIR, 'img')\n",
    "DEEP_FASHION_CLOTHING_CATEGORIES_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_category_cloth.txt')\n",
    "DEEP_FASHION_CLOTHING_ATTRIBUTES_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_attr_cloth.txt')\n",
    "DEEP_FASHION_CLOTHING_LIST_CAT_IMG_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_category_img.txt')\n",
    "DEEP_FASHION_CLOTHING_LIST_ATT_IMG_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_attr_img.txt')\n",
    "\n",
    "CLASS_LABELS = []\n",
    "with open(DEEP_FASHION_CLOTHING_LIST_CAT_IMG_PATH, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            CLASS_LABELS.append((int(line.split()[1]) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17292485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0cdf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154c4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hist = np.histogram(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4bf997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 38, 39, 40, 41, 42, 43, 45, 46, 47])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20f1c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.6000e+02, 7.4950e+03, 2.4557e+04, 3.0900e+02, 3.3000e+02,\n",
       "        1.3311e+04, 3.2400e+02, 1.7000e+01, 7.1600e+02, 4.0480e+03,\n",
       "        1.0467e+04, 7.4800e+02, 6.7600e+02, 9.7000e+01, 7.9100e+02,\n",
       "        1.3123e+04, 0.0000e+00, 1.5429e+04, 3.6887e+04, 1.0078e+04,\n",
       "        1.4600e+02, 7.7000e+01, 5.2700e+02, 4.8600e+02, 1.6690e+03,\n",
       "        4.9000e+01, 7.0760e+03, 5.9400e+02, 4.5000e+01, 4.4160e+03,\n",
       "        5.0130e+03, 3.2000e+01, 1.9666e+04, 0.0000e+00, 1.4773e+04,\n",
       "        3.0480e+03, 1.1060e+03, 3.8600e+02, 5.4000e+01, 0.0000e+00,\n",
       "        2.1200e+03, 1.7000e+01, 7.2158e+04, 6.1530e+03, 1.2600e+02,\n",
       "        2.2940e+03, 0.0000e+00, 7.0000e+01, 1.5000e+02, 7.4080e+03]),\n",
       " array([ 0.  ,  0.94,  1.88,  2.82,  3.76,  4.7 ,  5.64,  6.58,  7.52,\n",
       "         8.46,  9.4 , 10.34, 11.28, 12.22, 13.16, 14.1 , 15.04, 15.98,\n",
       "        16.92, 17.86, 18.8 , 19.74, 20.68, 21.62, 22.56, 23.5 , 24.44,\n",
       "        25.38, 26.32, 27.26, 28.2 , 29.14, 30.08, 31.02, 31.96, 32.9 ,\n",
       "        33.84, 34.78, 35.72, 36.66, 37.6 , 38.54, 39.48, 40.42, 41.36,\n",
       "        42.3 , 43.24, 44.18, 45.12, 46.06, 47.  ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = 'fashion-retrieval'\n",
    "ENTITY = None\n",
    "\n",
    "DEEP_FASHION_DIR = osp.join(osp.expanduser('~'), 'Documents', 'dev', 'DeepFashion')\n",
    "DEEP_FASHION_CLOTHING_ANNOS_DIR = osp.join(DEEP_FASHION_DIR, 'Anno_coarse')\n",
    "DEEP_FASHION_CLOTHING_IMAGES_DIR = osp.join(DEEP_FASHION_DIR, 'img')\n",
    "DEEP_FASHION_CLOTHING_CATEGORIES_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_category_cloth.txt')\n",
    "DEEP_FASHION_CLOTHING_ATTRIBUTES_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_attr_cloth.txt')\n",
    "DEEP_FASHION_CLOTHING_LIST_CAT_IMG_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_category_img.txt')\n",
    "DEEP_FASHION_CLOTHING_LIST_ATT_IMG_PATH = osp.join(DEEP_FASHION_CLOTHING_ANNOS_DIR, 'list_attr_img.txt')\n",
    "\n",
    "CLASS_LABELS = []\n",
    "ATTR_LABELS = []\n",
    "\n",
    "# 1. In category type, \"1\" represents upper-body clothes, \"2\" represents lower-body clothes, \"3\" represents full-body clothes;\n",
    "# 2. The order of category labels accords with the order of category names;\n",
    "# 3. In category labels, the number represents the category id in category names;\n",
    "# 4. For the clothing categories, \"Cape\", \"Nightdress\", \"Shirtdress\" and \"Sundress\" have been merged into \"Dress\";\n",
    "# 5. Category prediction is treated as a 1-of-K classification problem.\n",
    "\n",
    "CATEGORY_TYPES = {\n",
    "    1: 'upper-body',\n",
    "    2: 'lower-body',\n",
    "    3: 'full-body'\n",
    "}\n",
    "\n",
    "# 1. In attribute type, \"1\" represents texture-related attributes, \"2\" represents fabric-related attributes, \"3\" represents shape-related attributes, \"4\" represents part-related attributes, \"5\" represents style-related attributes;\n",
    "# 2. The order of attribute labels accords with the order of attribute names;\n",
    "# 3. In attribute labels, \"1\" represents positive while \"-1\" represents negative, '0' represents unknown;\n",
    "# 4. Attribute prediction is treated as a multi-label tagging problem.\n",
    "\n",
    "ATTRIBUTE_TYPES = {\n",
    "    1: 'texture',\n",
    "    2: 'fabric',\n",
    "    3: 'shape',\n",
    "    4: 'part',\n",
    "    5: 'style'\n",
    "}\n",
    "\n",
    "with open(DEEP_FASHION_CLOTHING_CATEGORIES_PATH, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            CLASS_LABELS.append(line.split()[0])\n",
    "            \n",
    "with open(DEEP_FASHION_CLOTHING_ATTRIBUTES_PATH, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            ATTR_LABELS.append(' '.join(l for l in line.split() if l.replace('-', '').isalpha()))\n",
    "            \n",
    "IDX_TO_CLASS = {idx + 1:c for idx, c in enumerate(CLASS_LABELS)}\n",
    "CLS_TO_IDX = {v: k for k, v in IDX_TO_CLASS.items()}\n",
    "\n",
    "DATA_DICT = dict()\n",
    "\n",
    "with open(DEEP_FASHION_CLOTHING_LIST_CAT_IMG_PATH, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            DATA_DICT[idx-2] = {\n",
    "                \"path\": line.split()[0],\n",
    "                \"cat_index\": int(line.split()[1]),\n",
    "                \"category\": IDX_TO_CLASS[int(line.split()[1])]\n",
    "            }\n",
    "\n",
    "with open(DEEP_FASHION_CLOTHING_LIST_ATT_IMG_PATH, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            DATA_DICT[idx-2]['attributes'] = [idx for idx, att in enumerate(line.split()[1:]) if int(att) > 0]  \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ca864",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_ATTR = {idx: val for idx, val in enumerate(ATTR_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a92ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ec113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initiate a wandb run\n",
    "run = wandb.init(project=WANDB_PROJECT, entity=ENTITY, job_type='upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c879da",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.Artifact(name='deep-fashion', type='raw-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3012fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_FASHION_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdd828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artifact.add_file(osp.join(DEEP_FASHION_DIR, 'README.txt'))\n",
    "# artifact.add_dir(DEEP_FASHION_CLOTHING_IMAGES_DIR, name='Images')\n",
    "# artifact.add_dir(DEEP_FASHION_CLOTHING_ANNOS_DIR, name='Annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af16777",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c1a23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run = wandb.init(project=WANDB_PROJECT, entity=ENTITY, job_type='upload')\n",
    "artifact = run.use_artifact('deep-fashion:latest',\n",
    "                            type='raw-data')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.sample(range(0, len(DATA_DICT.keys())), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=['Image Name', 'Image', 'Category', 'Attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _i in idx:\n",
    "    data = DATA_DICT.get(_i)\n",
    "    _name = data['path'].split('/')[1]\n",
    "    _img = Image.open(osp.join(DEEP_FASHION_DIR, data['path']))\n",
    "    _cat = data['category']\n",
    "    _attrs = [IDX_TO_ATTR[attr] for attr in data['attributes']]\n",
    "    table.add_data(_name,\n",
    "                  wandb.Image(_img),\n",
    "                  _cat,\n",
    "                  _attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46754b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eda_artifact = wandb.Artifact('deep-fashion', type='raw-data')\n",
    "eda_artifact.add(table, name='EDA Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(eda_artifact)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fc303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = wandb.Table(columns=['Image Name', 'Image', 'Category', 'Attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff628fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, val in DATA_DICT:\n",
    "#     img = Image.open(osp.join())\n",
    "#     table.add_data(val['path'].split('/')[1],\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af86b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_mask(img_shape, segmentation_map_list, category):\n",
    "    mask = np.zeros(img_shape[1]*img_shape[0], dtype=int)\n",
    "    return mask\n",
    "\n",
    "def get_polygon_regions(segm_mask):\n",
    "    return [[pair for pair in zip(region[::2], region[1::2])] for region in segm_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e42515",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_FASHION_TRAIN_DIR = os.path.join(DEEP_FASHION_DIR, 'train')\n",
    "DEEP_FASHION_VALIDATION_DIR = os.path.join(DEEP_FASHION_DIR, 'validation')\n",
    "DEEP_FASHION_TEST_DIR = os.path.join(DEEP_FASHION_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos = sorted(os.listdir(Path(DEEP_FASHION_TRAIN_DIR) / 'annos'))\n",
    "train_images = sorted(os.listdir(Path(DEEP_FASHION_TRAIN_DIR) / 'image'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annos_paths = list(map(lambda x: os.path.join(DEEP_FASHION_TRAIN_DIR, 'annos', x), train_annos))\n",
    "train_images_paths = list(map(lambda x: os.path.join(DEEP_FASHION_TRAIN_DIR, 'image', x), train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bed95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(train_annos_paths[0], 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These points define a polygon\n",
    "segm = data['item1']['segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece24251",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 257, 35\n",
    "nn[y*w + x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.reshape(h, w)[35, 257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(train_images_paths[0]).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = get_polygon_regions(segm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for poly in polys:\n",
    "    ImageDraw.Draw(img).polygon(poly, outline=1, fill=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61093bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76beb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7f8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_fashion_dir = config.DEEP_FASHION_DIR\n",
    "annos_dir = osp.join(deep_fashion_dir, 'Anno_coarse')\n",
    "bboxes_path = osp.join(annos_dir, 'list_bbox.txt')\n",
    "attributes_path = osp.join(annos_dir, 'list_attr_img.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c232b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(attributes_path, 'r') as f:\n",
    "    lines = f.readlines()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bd0d",
   "metadata": {},
   "source": [
    "## Time for executing with only stripping and splitting:\n",
    "\n",
    "`CPU times: user 38.3 s, sys: 47.1 s, total: 1min 25s\n",
    "Wall time: 1min 30s`\n",
    "\n",
    "## Time by only storing the line content\n",
    "`CPU times: user 9.13 s, sys: 15.6 s, total: 24.8 s\n",
    "Wall time: 25 s`\n",
    "\n",
    "## Time w/o using `for` loop\n",
    "`atts.append(list(filter(lambda x: x != '', list(map(lambda line: line.strip().split(' ')[1:], lines[2:])) )))`\n",
    "\n",
    "`CPU times: user 43.1 s, sys: 1min 2s, total: 1min 45s\n",
    "Wall time: 1min 56s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7807ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.1 s, sys: 1min 2s, total: 1min 45s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "atts = []\n",
    "\n",
    "atts.append(list(filter(lambda x: x != '', list(map(lambda line: line.strip().split(' ')[1:], lines[2:])) )))\n",
    "# for line in lines[2:]:\n",
    "#     atts.append(line)\n",
    "#     atts.append(list(filter(lambda x: x != '', line.strip().split(' ')[1:])))\n",
    "#     atts.append(list(map(lambda x: int(x), list(filter(lambda x: x != '', line.strip().split(' ')[1:])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ddda658",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(map(lambda x: x.strip(), lines[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fcc99280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('                        ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b80db578",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bbox = []\n",
    "with open(bboxes_path, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            line_split = list(map(int, line.strip().split(' ')[24:]))\n",
    "            bbox.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10ad6d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['072', '079', '232', '273']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3b40b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bboxes_path, 'r') as f:\n",
    "    lines = f.readlines()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52131780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b18da1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 48.3 s, total: 1min 3s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bbox = list( list(filter(lambda x: x != '' , list(map(lambda line: line.strip().split(' ')[1:], lines))) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eeb40ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == ' ', lines[0].split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "821e4617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img/Sheer_Pleated-Front_Blouse/img_00000001.jpg072079232273\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0].replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "528f93fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 19.3 s, total: 49.1 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "atts = []\n",
    "\n",
    "with open(attributes_path, 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 1:\n",
    "            atts.append(line.strip().split(' ')[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "659bd24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289222"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f960df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /home/kaseris/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
      "14.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "51.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n",
      "/home/kaseris/Documents/envs/wandb-fashion-retr/lib/python3.10/site-packages/torchvision/utils.py:232: UserWarning: Argument 'font_size' will be ignored since 'font' is not set.\n",
      "  warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"/home/kaseris/Documents/fir/11.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4, font_size=30)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd692318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[   9.7663,   54.3057,  825.1075, 1479.4414],\n",
       "         [  86.8136,    0.0000,  834.8762,  835.5492]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'labels': tensor([1, 1]),\n",
       " 'scores': tensor([0.9989, 0.9026], grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0bdc23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
